{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import re\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "def get_document(doc_id, base_path=\"/root/data/msmarco_v2.1_doc_segmented/\"):\n",
    "    match = re.match(r'msmarco_v2\\.1_doc_(\\d+)_(\\d+)#(\\d+)_(\\d+)', doc_id)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid doc_id format: {doc_id}\")\n",
    "    \n",
    "    shard_number = int(match.group(1))\n",
    "    byte_offset = int(match.group(4))\n",
    "    file_path = os.path.join(base_path, f\"msmarco_v2.1_doc_segmented_{shard_number:02d}.json.gz\")\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        f.seek(byte_offset)\n",
    "        line = f.readline().decode('utf-8')\n",
    "        \n",
    "        try:\n",
    "            document = json.loads(line)\n",
    "            if document['docid'] == doc_id:\n",
    "                return document\n",
    "            else:\n",
    "                raise ValueError(f\"Document at offset does not match requested doc_id: {doc_id}\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(f\"Invalid JSON at offset {byte_offset} in file {file_path}\")\n",
    "\n",
    "def get_document_wrapper(doc_id):\n",
    "    try:\n",
    "        document = get_document(doc_id)\n",
    "        return {'docid': doc_id, 'content': document['segment']}\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving document {doc_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def rerank_results(query, results, batch_size=32):\n",
    "    with multiprocessing.Pool(processes=16) as pool:\n",
    "        documents = list(tqdm.tqdm(\n",
    "            pool.imap(get_document_wrapper, [result['doc_id'] for result in results]),\n",
    "            total=len(results),\n",
    "            desc=\"Gathering documents\"\n",
    "        ))\n",
    "    \n",
    "    documents = [doc for doc in documents if doc is not None]\n",
    "    \n",
    "    all_scores = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        sentence_pairs = [[query, doc['content']] for doc in batch]\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            scores = model.compute_score(sentence_pairs, max_length=1024)\n",
    "        \n",
    "        all_scores.extend(scores.tolist())\n",
    "    \n",
    "    # Combine scores with original results and sort\n",
    "    for result, score in zip(results[:len(all_scores)], all_scores):\n",
    "        result['new_score'] = score\n",
    "    \n",
    "    reranked_results = sorted(results[:len(all_scores)], key=lambda x: x['new_score'], reverse=True)\n",
    "    return reranked_results\n",
    "def parse_results(results_string):\n",
    "    parsed_results = []\n",
    "    for line in results_string.strip().split('\\n'):\n",
    "        parts = line.split()\n",
    "        parsed_results.append({\n",
    "            'query_id': parts[0],\n",
    "            'q0': parts[1],\n",
    "            'doc_id': parts[2],\n",
    "            'rank': int(parts[3]),\n",
    "            'score': float(parts[4]),\n",
    "            'run_id': parts[5]\n",
    "        })\n",
    "    return parsed_results\n",
    "\n",
    "# Main reranking function\n",
    "def rerank_and_save(input_file, output_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        input_data = f.read()\n",
    "    \n",
    "    results = parse_results(input_data)\n",
    "    \n",
    "    # Group results by query_id\n",
    "    grouped_results = {}\n",
    "    for result in results:\n",
    "        query_id = result['query_id']\n",
    "        if query_id not in grouped_results:\n",
    "            grouped_results[query_id] = []\n",
    "        grouped_results[query_id].append(result)\n",
    "    \n",
    "    # Rerank each group\n",
    "    reranked_groups = {}\n",
    "    for query_id, group in tqdm.tqdm(grouped_results.items()):\n",
    "        query = f\"Query for {query_id}\"  # Replace with actual query if available\n",
    "        reranked_groups[query_id] = rerank_results(query, group)\n",
    "    \n",
    "    # Write reranked results\n",
    "    with open(output_file, 'w') as f:\n",
    "        for query_id, reranked_results in reranked_groups.items():\n",
    "            for new_rank, result in enumerate(reranked_results, start=1):\n",
    "                f.write(f\"{query_id} Q0 {result['doc_id']} {new_rank} {result['new_score']:.8f} cross-encoder\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(1026, 768)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "    )\n",
       "    (emb_drop): Dropout(p=0.1, inplace=False)\n",
       "    (emb_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (mixer): MHA(\n",
       "            (Wqkv): LinearResidual(in_features=768, out_features=2304, bias=True)\n",
       "            (inner_attn): SelfAttention(\n",
       "              (drop): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (inner_cross_attn): CrossAttention(\n",
       "              (drop): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'jinaai/jina-reranker-v2-base-multilingual',\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.to('cuda')  # Use 'cpu' if no GPU is available\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/301 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering documents: 100%|██████████| 125/125 [01:04<00:00,  1.94it/s]\n",
      "  0%|          | 0/301 [01:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_results.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreranked_results.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrerank_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 99\u001b[0m, in \u001b[0;36mrerank_and_save\u001b[0;34m(input_file, output_file)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query_id, group \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(grouped_results\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m     98\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with actual query if available\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     reranked_groups[query_id] \u001b[38;5;241m=\u001b[39m \u001b[43mrerank_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Write reranked results\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[8], line 58\u001b[0m, in \u001b[0;36mrerank_results\u001b[0;34m(query, results, batch_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m     56\u001b[0m         scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_score(sentence_pairs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m     all_scores\u001b[38;5;241m.\u001b[39mextend(\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m())\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Combine scores with original results and sort\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results[:\u001b[38;5;28mlen\u001b[39m(all_scores)], all_scores):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "input_file = \"test_results.txt\"\n",
    "output_file = \"reranked_results.txt\"\n",
    "rerank_and_save(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "def rerank_results(query, results, batch_size=32):\n",
    "    print(f\"Reranking for query: {query}\")\n",
    "    print(f\"Number of results: {len(results)}\")\n",
    "    \n",
    "    with multiprocessing.Pool(processes=32) as pool:\n",
    "        documents = list(tqdm.tqdm(\n",
    "            pool.imap(get_document_wrapper, [result['doc_id'] for result in results]),\n",
    "            total=len(results),\n",
    "            desc=\"Gathering documents\"\n",
    "        ))\n",
    "    \n",
    "    documents = [doc for doc in documents if doc is not None]\n",
    "    print(f\"Number of valid documents: {len(documents)}\")\n",
    "    \n",
    "    all_scores = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        sentence_pairs = [[query, doc['content']] for doc in batch]\n",
    "        \n",
    "        print(f\"Processing batch {i//batch_size + 1}, size: {len(batch)}\")\n",
    "        with torch.no_grad():\n",
    "            scores = model.compute_score(sentence_pairs, max_length=1024)\n",
    "        \n",
    "        print(f\"Scores type: {type(scores)}\")\n",
    "        if isinstance(scores, list):\n",
    "            print(f\"Scores length: {len(scores)}\")\n",
    "            all_scores.extend(scores)\n",
    "        else:\n",
    "            print(f\"Scores shape: {scores.shape}\")\n",
    "            all_scores.extend(scores.squeeze().tolist())\n",
    "    \n",
    "    print(f\"Total scores: {len(all_scores)}\")\n",
    "    \n",
    "    # Combine scores with original results and sort\n",
    "    for result, score in zip(results[:len(all_scores)], all_scores):\n",
    "        result['new_score'] = score\n",
    "    \n",
    "    reranked_results = sorted(results[:len(all_scores)], key=lambda x: x['new_score'], reverse=True)\n",
    "    return reranked_results\n",
    "# ... (keep the existing parse_results function)\n",
    "def parse_results(results_string):\n",
    "    parsed_results = []\n",
    "    for line in results_string.strip().split('\\n'):\n",
    "        parts = line.split()\n",
    "        parsed_results.append({\n",
    "            'query_id': parts[0],\n",
    "            'q0': parts[1],\n",
    "            'doc_id': parts[2],\n",
    "            'rank': int(parts[3]),\n",
    "            'score': float(parts[4]),\n",
    "            'run_id': parts[5]\n",
    "        })\n",
    "    return parsed_results\n",
    "\n",
    "\n",
    "def rerank_and_save(input_file, output_file, num_queries=3):\n",
    "    with open(input_file, 'r') as f:\n",
    "        input_data = f.read()\n",
    "    \n",
    "    results = parse_results(input_data)\n",
    "    \n",
    "    # Group results by query_id\n",
    "    grouped_results = {}\n",
    "    for result in results:\n",
    "        query_id = result['query_id']\n",
    "        if query_id not in grouped_results:\n",
    "            grouped_results[query_id] = []\n",
    "        grouped_results[query_id].append(result)\n",
    "    \n",
    "    # Rerank a subset of queries\n",
    "    reranked_groups = {}\n",
    "    for query_id, group in list(grouped_results.items())[:num_queries]:\n",
    "        print(f\"\\nProcessing query_id: {query_id}\")\n",
    "        query = f\"Query for {query_id}\"  # Replace with actual query if available\n",
    "        reranked_groups[query_id] = rerank_results(query, group)\n",
    "    \n",
    "    # Write reranked results\n",
    "    with open(output_file, 'w') as f:\n",
    "        for query_id, reranked_results in reranked_groups.items():\n",
    "            for new_rank, result in enumerate(reranked_results, start=1):\n",
    "                f.write(f\"{query_id} Q0 {result['doc_id']} {new_rank} {result['new_score']:.8f} cross-encoder\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query_id: 2024-145979\n",
      "Reranking for query: Query for 2024-145979\n",
      "Number of results: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering documents: 100%|██████████| 125/125 [00:46<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid documents: 125\n",
      "Processing batch 1, size: 32\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 32\n",
      "Processing batch 2, size: 32\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 32\n",
      "Processing batch 3, size: 32\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 32\n",
      "Processing batch 4, size: 29\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 29\n",
      "Total scores: 125\n",
      "\n",
      "Processing query_id: 2024-36935\n",
      "Reranking for query: Query for 2024-36935\n",
      "Number of results: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering documents: 100%|██████████| 125/125 [00:36<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid documents: 125\n",
      "Processing batch 1, size: 32\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 32\n",
      "Processing batch 2, size: 32\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 32\n",
      "Processing batch 3, size: 32\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 32\n",
      "Processing batch 4, size: 29\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 29\n",
      "Total scores: 125\n",
      "\n",
      "Processing query_id: 2024-216592\n",
      "Reranking for query: Query for 2024-216592\n",
      "Number of results: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering documents: 100%|██████████| 125/125 [00:37<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid documents: 125\n",
      "Processing batch 1, size: 32\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 32\n",
      "Processing batch 2, size: 32\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 32\n",
      "Processing batch 3, size: 32\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 32\n",
      "Processing batch 4, size: 29\n",
      "Scores type: <class 'list'>\n",
      "Scores length: 29\n",
      "Total scores: 125\n"
     ]
    }
   ],
   "source": [
    "input_file = \"test_results.txt\"\n",
    "output_file = \"reranked_results.txt\"\n",
    "rerank_and_save(input_file, output_file, num_queries=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
